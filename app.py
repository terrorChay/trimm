import streamlit as st
import utils as utils
from bs4 import BeautifulSoup
import requests
import pandas as pd
import re
from io import BytesIO

regions = {
    '–ê—Å—Ç—Ä–∞—Ö–∞–Ω—å' : 'https://prodoctorov.ru/astrahan/lpu',
    '–°–æ—á–∏'      : 'https://prodoctorov.ru/sochi/lpu',
    '–¢—é–º–µ–Ω—å'    : 'https://prodoctorov.ru/tyumen/lpu',
    '–í–æ—Ä–æ–Ω–µ–∂'   : 'https://prodoctorov.ru/voronezh/lpu',
}

@st.experimental_memo
def scrape(address):
    # Define things to find
    target_attrs = {
        '–ù–∞–∑–≤–∞–Ω–∏–µ'      : ('span', {"data-qa": "lpu_card_heading_lpu_name"}),
        '–¢–∏–ø'           : ('div', {"data-qa": "lpu_card_subheading_lputype_name"}),
        '–ö–æ–ª-–≤–æ –≤—Ä–∞—á–µ–π' : ('div', {"data-qa": "lpu_card_subheading_doctors_count"}),
        '–ê–¥—Ä–µ—Å'         : ('span', {"data-qa": "lpu_card_btn_addr_text"}),
        '–¢–µ–ª–µ—Ñ–æ–Ω'       : ('span', {"data-qa": "lpu_card_btn_phone_text"}),
        '–û—Ç–∫—Ä—ã—Ç–æ –¥–æ'    : ('span', {"data-qa": "lpu_card_btn_schedule_text"}),
        '–¶–µ–Ω—ã'          : ('span', {"data-qa": "lpu_card_btn_prices_num"}),
        '–û—Ç–∑—ã–≤—ã'        : ('span', {"data-qa": "lpu_card_stars_text"}),
    }

    # Output dataframe to fil
    df = pd.DataFrame(columns=['–ù–∞–∑–≤–∞–Ω–∏–µ', '–¢–∏–ø', '–ö–æ–ª-–≤–æ –≤—Ä–∞—á–µ–π', '–ê–¥—Ä–µ—Å', '–¢–µ–ª–µ—Ñ–æ–Ω', '–û—Ç–∫—Ä—ã—Ç–æ –¥–æ', '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ —Ü–µ–Ω', '–û—Ç–∑—ã–≤–æ–≤'])

    page_num = 1
    response = '200'
    while response == '200':
        url = f"{address}/?page={page_num}"
        page = requests.get(url)
        response = str(page.status_code)
        if response == '200':
            page_num += 1
            soup = BeautifulSoup(page.text, "html.parser")
            # Iterate over clinic cards
            allLPU = soup.findAll('div', class_='b-card__row')
            for item in allLPU:
                item_data = []
                for key, attrs in target_attrs.items():
                    data_unit = item.find(*attrs)
                    if data_unit is not None:
                        raw_text = data_unit.text.strip("""\n               """)
                        if key == '–ö–æ–ª-–≤–æ –≤—Ä–∞—á–µ–π' or key == '–û—Ç–∑—ã–≤—ã':
                            raw_text = re.sub(r"\D", "", raw_text)
                        item_data.append(raw_text)
                    else:
                        item_data.append(data_unit)
                df.loc[len(df)] = item_data
    st.success(f'–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {page_num-1} —Å—Ç—Ä–∞–Ω–∏—Ü! –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num} –≤–µ—Ä–Ω—É–ª–∞ –æ—à–∏–±–∫—É {response}.')
    return df

@st.experimental_memo
def convert_df(df: pd.DataFrame, to_excel=False):
    if to_excel:
        output = BytesIO()
        writer = pd.ExcelWriter(output, engine='xlsxwriter')
        df.to_excel(writer, index=False, sheet_name='ilya-matyushin')
        workbook = writer.book
        worksheet = writer.sheets['ilya-matyushin']
        format1 = workbook.add_format({'num_format': '0.00'}) 
        worksheet.set_column('A:A', None, format1)  
        workbook.close()
        processed_data = output.getvalue()
    else:
        processed_data = df.to_csv().encode('utf-8')
    return processed_data

def main():
    st.header('prodoctorov.ru –õ–ü–£ –ø–∞—Ä—Å–µ—Ä')
    with st.form('parser'):
        region = st.selectbox('–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–≥–∏–æ–Ω', ['–ê—Å—Ç—Ä–∞—Ö–∞–Ω—å', '–°–æ—á–∏', '–¢—é–º–µ–Ω—å', '–í–æ—Ä–æ–Ω–µ–∂'])
        address = regions[region]
        submit = st.form_submit_button('–ü–æ–µ—Ö–∞–ª–∏')
    if submit:
        df = scrape(address)
        st.dataframe(df)
        st.download_button('üíæ Excel', data=convert_df(df, True), file_name=f"{region}.xlsx")

if __name__ == "__main__":
    utils.page_config(layout='centered', title='matyush.in')
    utils.remove_footer()
    main()