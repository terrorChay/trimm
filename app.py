import streamlit as st
import utils as utils
from bs4 import BeautifulSoup
import requests
import pandas as pd
import re
from io import BytesIO

regions = {
    '–ê—Å—Ç—Ä–∞—Ö–∞–Ω—å' : 'https://prodoctorov.ru/astrahan/',
    '–°–æ—á–∏'      : 'https://prodoctorov.ru/sochi/',
    '–¢—é–º–µ–Ω—å'    : 'https://prodoctorov.ru/tyumen/',
    '–í–æ—Ä–æ–Ω–µ–∂'   : 'https://prodoctorov.ru/voronezh/',
}

@st.experimental_memo
def scrape(address, find_lpu, page_limit):
    # Define things to find
    if find_lpu:
        target_attrs = {
            '–ù–∞–∑–≤–∞–Ω–∏–µ'      : ('span', {"data-qa": "lpu_card_heading_lpu_name"}),
            '–¢–∏–ø'           : ('div', {"data-qa": "lpu_card_subheading_lputype_name"}),
            '–ö–æ–ª-–≤–æ –≤—Ä–∞—á–µ–π' : ('div', {"data-qa": "lpu_card_subheading_doctors_count"}),
            '–ê–¥—Ä–µ—Å'         : ('span', {"data-qa": "lpu_card_btn_addr_text"}),
            '–¢–µ–ª–µ—Ñ–æ–Ω'       : ('span', {"data-qa": "lpu_card_btn_phone_text"}),
            '–û—Ç–∫—Ä—ã—Ç–æ –¥–æ'    : ('span', {"data-qa": "lpu_card_btn_schedule_text"}),
            '–¶–µ–Ω—ã'          : ('span', {"data-qa": "lpu_card_btn_prices_num"}),
            '–û—Ç–∑—ã–≤—ã'        : ('span', {"data-qa": "lpu_card_stars_text"}),
        }
    else:
        target_attrs = {
            '–§–ò–û'               : ('span', {"class": "b-doctor-card__name-surname"}),
            '–°–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å'     : ('div', {"class": "b-doctor-card__spec"}),
            '–°—Ç–∞–∂'              : ('div', {"class": "b-doctor-card__experience-years"}),
            '–ö–∞—Ç–µ–≥–æ—Ä–∏—è'         : ('div', {"class": "b-doctor-card__category"}),
            '–û—Ç–∑—ã–≤–æ–≤'           : ('a', {"class": "ui-text ui-text_body-2 b-link b-link_prg b-link_color_grey b-link_underline"}),
            '–ö–ª–∏–Ω–∏–∫–∞'           : ('span', {"class": "b-select__trigger-main-text"}),
            '–ê–¥—Ä–µ—Å –∫–ª–∏–Ω–∏–∫–∏'     : ('span', {"class": "b-select__trigger-adit-text"}),
        }

    # Output dataframe to fil
    df = pd.DataFrame(columns=target_attrs.keys())

    page_num = 1
    response = '200'
    while response == '200' and page_num-1 != page_limit:
        url = f"{address}/?page={page_num}"
        page = requests.get(url)
        response = str(page.status_code)
        if response == '200':
            page_num += 1
            soup = BeautifulSoup(page.text, "html.parser")
            # Iterate over
            if find_lpu:
                all = soup.findAll('div', class_='b-card__row')
            else:
                all = soup.findAll('div', class_='b-doctor-card')
            for item in all:
                item_data = []
                for key, attrs in target_attrs.items():
                    data_unit = item.find(*attrs)
                    if data_unit is not None:
                        raw_text = data_unit.text.strip("""\n               """)
                        if key == '–ö–æ–ª-–≤–æ –≤—Ä–∞—á–µ–π' or key == '–û—Ç–∑—ã–≤—ã':
                            raw_text = re.sub(r"\D", "", raw_text)
                        elif key == '–°–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å':
                            raw_text = ', '.join(list(map(lambda x: x.strip() , raw_text.split(','))))
                        item_data.append(raw_text)
                    else:
                        item_data.append(data_unit)
                df.loc[len(df)] = item_data
    st.success(f'–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {page_num-1} —Å—Ç—Ä–∞–Ω–∏—Ü! –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num} –≤–µ—Ä–Ω—É–ª–∞ –∫–æ–¥ {response}.')
    return df

@st.experimental_memo
def convert_df(df: pd.DataFrame, to_excel=False):
    if to_excel:
        output = BytesIO()
        writer = pd.ExcelWriter(output, engine='xlsxwriter')
        df.to_excel(writer, index=False, sheet_name='ilya-matyushin')
        workbook = writer.book
        worksheet = writer.sheets['ilya-matyushin']
        format1 = workbook.add_format({'num_format': '0.00'}) 
        worksheet.set_column('A:A', None, format1)  
        workbook.close()
        processed_data = output.getvalue()
    else:
        processed_data = df.to_csv().encode('utf-8')
    return processed_data

def main():
    st.subheader('ilya@matyush.in')
    with st.form('parser'):
        region = st.selectbox('–ì–¥–µ –∏—â–µ–º?', ['–ê—Å—Ç—Ä–∞—Ö–∞–Ω—å', '–°–æ—á–∏', '–¢—é–º–µ–Ω—å', '–í–æ—Ä–æ–Ω–µ–∂'])
        to_find = st.selectbox('–ß—Ç–æ –∏—â–µ–º?', ['–õ–ü–£', '–í—Ä–∞—á–∏'])
        page_limit = st.select_slider('–ú–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü', options=['–ù–µ—Ç']+list(range(1,21)))
        address = regions[region]
        submit = st.form_submit_button('–ü–æ–µ—Ö–∞–ª–∏')
    if submit:
        if to_find == '–õ–ü–£':
            df = scrape(address+'lpu', True, page_limit)
        else:
            df = scrape(address+'vrach', False, page_limit)
        st.dataframe(df)
        st.download_button('üíæ Excel', data=convert_df(df, True), file_name=f"{region}.xlsx")

if __name__ == "__main__":
    utils.page_config(layout='centered', title='matyush.in')
    utils.remove_footer()
    main()